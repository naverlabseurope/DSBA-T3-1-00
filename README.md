# Goal
The goal of this exercise is to analyse T5 model performance from different aspects
 - inference
 - interpertability

# T5 model
 - how to get if from Hugging face
 - to know more about it: 
 Stanford guest lecture about T5: http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture14-t5.pdf
 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (https://arxiv.org/pdf/1910.10683.pdf)

## Datasets
- Check T5 tasks datasets

## Evaluations
- Summarizaiton (ROUGE) 
- MT (BLEU) 
- evaluate the metrics or use existing implementations?
 
# Task 1 Evaluation metrics (5 pt)
 - implement all the necessary evaluation metrics : BLEU, ROUGE
 - Optional: implement additional evaluation metrics for translation and/or summarization (bonus: 5 pt)
 
# Task 1 Inference (8 pt)
link to (notebook1)
- Implement Softmax with Temp / Nucleus / Beam Search
- T5 take all the tasks write a report on how different decoding algorithms with T5
 
## Questions

# Task 2 Attention Visualization (7 pt) 
- Adapt the Transformers (@vassilina to show encoder-decoder attention)
- All Tasks for T5
- Visualizing attention: What is the difference in cross attention in different tasks. 
- Show different patterns in different Heads (For this you will need to implement head confidence) 


# Deliverables


## Task 1 
- Report (Answering Questions) Tables of comparison 
- Link to a colab notebook


## T5 tasks 
Present a short summary on T5  (Answer some questions)
