## T5 model
 - how to get if from Hugging face
 - to know more about it: 
 Stanford guest lecture about T5: http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture14-t5.pdf
 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (https://arxiv.org/pdf/1910.10683.pdf)
 
# Task 1 Inference 
link to (notebook1)
- Implement Softmax with Temp / Nucleus / Beam Search
- T5 take all the tasks write a report on how different decoding algorithms with T5
 
## Datasets
- Check T5 tasks datasets

## Evaluations
- Summarizaiton (ROUGE) (Bonus extra eval. metrics) 
- MT (BLEU) (Bonus on extra eval. metrics)

## Questions

# Task 2 Attention Visualization 
- Adapt the Transformers (@vassilina to show encoder-decoder attention)
- All Tasks for T5
- Visualizing attention: What is the difference in cross attention in different tasks. 
- Show different patterns in different Heads (For this you will need to implement head confidence) 


# Deliverables


## Task 1 
- Report (Answering Questions) Tables of comparison 
- Link to a colab notebook


## T5 tasks 
Present a short summary on T5  (Answer some questions)
